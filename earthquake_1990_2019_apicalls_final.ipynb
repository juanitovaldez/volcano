{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQS and USGS Data Set Retrieval:\n",
    "## Dependencies and API setup\n",
    "### Goals and Target:\n",
    "    1. Load A Volcano event from the USGS Data Set into a data frame or json object\n",
    "    2. Retrieve and load AQS data for a timewindow around the volcano event from sensors deteremined by a bounding box of a pair of (lat,long) keys.\n",
    "    3. Retrieve only records for gasses we are interested in.\n",
    "    4. Set up helper functions and files to make scaling painless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T03:55:47.987662Z",
     "start_time": "2019-10-18T03:55:44.634864Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import geojson\n",
    "import json\n",
    "# AQS API Things\n",
    "from config import apiURL, apiUser, apiPassword, aqsParams\n",
    "#USGS API Things:\n",
    "from config import usgsURL, usgsStart, usgsFormat, usgsMinmag, usgsConus\n",
    "\n",
    "params = ','.join(list(aqsParams.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQS URL Construction and helper functions:\n",
    "1. Take gcs_coords and event_date from usgs earthquake api\n",
    "2. Format Date/time USGS uses iso8061 time specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T13:53:10.627296Z",
     "start_time": "2019-10-19T13:53:10.599947Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Define functions to create a bounding box a time window for the api call\n",
    "#  BBox takes an argument window which is how many square degrees our box will be:\n",
    "def date_coord_id(df):\n",
    "    date = df.Date\n",
    "    coord = df.GCS\n",
    "    ueid = df.Identifier\n",
    "    return date, coord, ueid\n",
    "\n",
    "def bbox(window, gcs_coords):\n",
    "    bbox = {'minlat': gcs_coords[0]-window,\n",
    "            'maxlat': gcs_coords[0]+window,\n",
    "            'minlon': gcs_coords[1]-window,\n",
    "            'maxlon': gcs_coords[1]+window}\n",
    "    bbox_url = f'&minlat={bbox[\"minlat\"]}&maxlat={bbox[\"maxlat\"]}&minlon={bbox[\"minlon\"]}&maxlon={bbox[\"maxlon\"]}'\n",
    "    return bbox_url\n",
    "#  t_window takes a datetime and a time length to return a start and end date a time length's apart\n",
    "def t_window(event_date, window_length):\n",
    "    window = timedelta(days=window_length)\n",
    "    event_time = time.strptime(event_date, '%Y%m%d')\n",
    "    event_time = datetime.datetime(*event_time[:3])\n",
    "    date_window = {'bdate': (event_time - window).strftime(\"%Y%m%d\"),\n",
    "                   'edate': (event_time + window).strftime(\"%Y%m%d\")}\n",
    "    tw_url = f'&bdate={date_window[\"bdate\"]}&edate={date_window[\"edate\"]}'\n",
    "    return tw_url\n",
    "\n",
    "#  calls the epa aqs api and attempts to aattach the unique quake identifier to each station record\n",
    "def aqs_api_call(event_date, gcs_coords, quake_df):\n",
    "    bbox_url = bbox(1.0, gcs_coords)\n",
    "    tw_url = t_window(event_date, 7)\n",
    "    aqs_url = f'{apiURL}param={params[:]}&email={apiUser}&key={apiPassword}{bbox_url}{tw_url}'\n",
    "    aqs_raw = requests.get(aqs_url)\n",
    "    if aqs_raw.status_code != 200:\n",
    "        return\n",
    "    aqs_raw = aqs_raw.json()\n",
    "    return aqs_raw\n",
    "\n",
    "def aqs_bulk_call(earthquake_df):\n",
    "    j=0\n",
    "    eqaq_raw = {}\n",
    "    for quake in earthquake_df:\n",
    "        \n",
    "       while j < len(earthquake_df)-1:\n",
    "            print(f'Getting Records for: {earthquake_df[\"Identifier\"][j]}')\n",
    "            eqaq_raw[earthquake_df['Identifier'][j]] = (aqs_api_call(earthquake_df['Date'][j], earthquake_df['GCS'][j], earthquake_df))\n",
    "            j+=1\n",
    "    return eqaq_raw\n",
    "\n",
    "\n",
    "#  When you use the bulk api call, this function fails as it expects a json object not a list of json objects\n",
    "def aqs_scrub(aqs_json):\n",
    "    target_data = []\n",
    "    for entry in aqs_json:\n",
    "        i = 0\n",
    "        if aqs_json[entry] is None:\n",
    "            continue\n",
    "        else:\n",
    "            while i <= len(aqs_json[entry]['Data'])-1.0:\n",
    "                county = aqs_json[entry]['Data'][i]['county']\n",
    "                station = aqs_json[entry]['Data'][i]['site_number']\n",
    "                parameter = aqs_json[entry]['Data'][i]['parameter']\n",
    "                p_id = aqs_json[entry]['Data'][i]['parameter_code']\n",
    "                measure_date = aqs_json[entry]['Data'][i]['date_local']\n",
    "                time = aqs_json[entry]['Data'][i]['time_local']\n",
    "                measurement = aqs_json[entry]['Data'][i]['sample_measurement']\n",
    "                measurement_unit = aqs_json[entry]['Data'][i]['units_of_measure']\n",
    "                #ueid = aqs_json[entry]['Data'][i]['q_ueid']\n",
    "                entry_dict = {\"q_ueid\":entry,\n",
    "                              \"Parameter\":parameter,\n",
    "                              \"P_id\":p_id,\n",
    "                              \"Station\":station,\n",
    "                              \"Date\":measure_date,\n",
    "                              \"County\":county,\n",
    "                              \"Measurement\":measurement,\n",
    "                              \"Unit\":measurement_unit}\n",
    "                target_data.append(entry_dict)\n",
    "                i+=1\n",
    "    target_df = pd.DataFrame(target_data)\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USGS URL Construction and helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:50:34.983310Z",
     "start_time": "2019-10-18T23:50:34.952324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define fucntions to querry USGS for earthquakes in the US with mag > 7\n",
    "# Return gcs_coords(epicenter in lat,long), event_date(), magnitude, uid, depth\n",
    "# In this case the parameters for the USGS data search were decided in advance and \n",
    "# are specified and hardcoded in the config file. All we need is a function\n",
    "# To create the api call from the specs, and reduce the data to our target parameters:\n",
    "def usgs_api_call():\n",
    "    usgs_url = f\"{usgsURL}{usgsFormat}{usgsStart}{usgsMinmag}{usgsConus['minlat']}{usgsConus['maxlat']}{usgsConus['minlon']}{usgsConus['maxlon']}\"\n",
    "    usgs_raw = requests.get(usgs_url)\n",
    "    usgs_raw = usgs_raw.json()\n",
    "    return usgs_raw\n",
    "\n",
    "#Exctract the information we're interested in stuff into a dataframe\n",
    "def usgs_scrub(usgs_geojson):\n",
    "    target_data = []\n",
    "    i=0\n",
    "    for entry in usgs_geojson['features']:\n",
    "        print(entry, i)\n",
    "        identifier = usgs_geojson['features'][i]['id']\n",
    "        mag = usgs_geojson['features'][i]['properties']['mag']\n",
    "        epochtime = usgs_geojson['features'][i]['properties']['time']\n",
    "        converted_date= time.strftime('%Y%m%d',  time.gmtime(epochtime/1000))\n",
    "        converted_time = time.strftime('%H:%M:%S',  time.gmtime(epochtime/1000))\n",
    "        place = usgs_geojson['features'][i]['properties']['place']\n",
    "        lon = float(usgs_geojson['features'][i]['geometry']['coordinates'][0])\n",
    "        lat = float(usgs_geojson['features'][i]['geometry']['coordinates'][1])\n",
    "        gcs_coords = [lat,lon]\n",
    "        depth = usgs_geojson['features'][i]['geometry']['coordinates'][2]\n",
    "        entry_dict = {\"Identifier\":identifier, \n",
    "                       \"Location\":place,\n",
    "                       \"GCS\": gcs_coords, \n",
    "                       \"Depth\":depth,\n",
    "                       \"Magnitude\":mag, \n",
    "                       \"Date\":converted_date,\n",
    "                       \"Time\":converted_time}\n",
    "        target_data.append(entry_dict)\n",
    "        i+=1\n",
    "    target_df = pd.DataFrame(target_data)\n",
    "    target_df = target_df[~target_df['Location'].str.contains('Mexico')]\n",
    "    target_df = target_df[~target_df['Location'].str.contains('MX')]\n",
    "\n",
    "    return target_df\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Calls \n",
    "1. Call USGS api first to get coordinates and date to feed into the AQS API\n",
    "2. Call AQS API with coordinates and date\n",
    "3. Clean AQS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USGS Call and get date and coordinates for next call:\n",
    "earthquake_df = usgs_scrub(usgs_api_call())\n",
    "earthquake_df\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This resets the index after we removed some values not in the US\n",
    "earthquake_df = earthquake_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T02:08:24.200768Z",
     "start_time": "2019-10-18T00:19:49.038930Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AQS Call: Un Comment out when you want to run the first time:\n",
    "#Bulk Call for when you get all the events you are interested\n",
    "# eqaq_json = aqs_bulk_call(earthquake_df)\n",
    "# Don't run this again it takes a long time 1h45m ish "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T02:23:05.237452Z",
     "start_time": "2019-10-18T02:20:32.629276Z"
    }
   },
   "outputs": [],
   "source": [
    "# save our json as a file for posterity\n",
    "# also just run once: commented out for safety\n",
    "\n",
    "#with open('epa_aq_mag5_7dbuff_1990_2019.json', 'w') as f:\n",
    "#   json.dump(eqaq_json, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aqs_list_o_df = []\n",
    "i = 0\n",
    "for record in edqaq_json:\n",
    "    aqs_list_o_df.append(aqs_scrub(eqaq_json[i]))\n",
    "    i+=1\n",
    "aqs_df = pd.concat(aqs_list_o_df).reset_index(drop=True)\n",
    "aqs_df.to_csv('epa_aq_bbox_1990_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:53:43.108853Z",
     "start_time": "2019-10-18T23:53:12.672260Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the file you saved as a json object into python\n",
    "with open(\"epa_aq_mag5_7dbuff_1990_2019.json\", \"r\") as read_file:\n",
    "    eqaq_json = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T13:53:29.196321Z",
     "start_time": "2019-10-19T13:53:18.968629Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scrub the data of the attributes we aren't interested in: defined above\n",
    "eqaq_df = aqs_scrub(eqaq_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T13:54:57.920858Z",
     "start_time": "2019-10-19T13:54:57.902009Z"
    }
   },
   "outputs": [],
   "source": [
    "eqaq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T13:55:06.162505Z",
     "start_time": "2019-10-19T13:54:57.923340Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save your hard scrapped data into a file to be passed off to cleaning and viz\n",
    "eqaq_df.to_csv('final_aq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
